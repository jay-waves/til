## 复杂度分析

数据结构和算法的核心需求是: 让程序更快, 同时更节省存储空间. 要衡量这两个指标, 一种方法是 `profiling`,  即不断运行该程序并监控性能和内存, 这种方法不够"理想", 影响因素多; 另一种方法就是 **复杂度分析**: 用 $n$ 模拟数据规模, 此时程序的时间或空间用量可表示为 $f(n)$. 当 $n$ 规模足够大时, 我们仅关心 $f(n)$ 的*上界*, 即: $f(n)=O(g(n))$

### 符号定义

$f(n)=\Theta(g(n))$, 当且仅当 $\exists c_1,c_2,n_0>0$, 使得 $\forall n \ge n_0, 0\le c_1\cdot g(n)\le f(n) \le c_2\cdot g(n)$.

$f(n)=O(g(n))$, 当且仅当 $\exists c,n_0$, 使得 $\forall n \ge n_0,0\le f(n)\le c\cdot g(n)$. 研究时间复杂度一般使用 $O$ 符号. $\Theta$ 无疑更精确, 但理论推导算法的下界有时很困难/复杂.

*这里的时间上界和最坏时间复杂度有区别*. $f(n)$ 在不同统计场景下会有不同定义, 如: 最乐观估计, 平均估计, 均摊估计, 最坏估计定义了不同 $f(n)$.

$f(n)=\Omega(g(n))$, 当且仅当 $\exists c,n_0$, 使得 $\forall n \ge n_0,0\le c\cdot g(n)\le f(n)$.

![图源 OI Wiki](../attach/Pasted%20image%2020240604200743.png)

### 实例

举例而言, 下述程序的时间复杂度为 $O(n^{2})$:
```c
for (i = 0; i < n; ++i) {
	for (j = 0; j <= n; ++j) {
		...
	}
}
```

复杂度为 $O(log_{2}(n))$ 的程序:

```c
i = 1;
while (i <= n) {
	i = i * 2;
}
```

复杂度为 $O(m+n)$ 的程序, 取决于 $m, n$ 的规模:

```c
for(i = 0; i < n; ++i)
	...

for(i = 0; i < m; ++i)
	...
```

### 最好与最坏情况时间复杂度

对于代码的概率性分支, 或分布不均匀的未知数据, 程序整体运行时间不固定, 存在最好与最坏执行情况 (best/worst case time complexity).

下面的样例代码在数组中查找值, 最好情况, `arr` 第一个元素就是 `x`, 立即返回, 即 `O(1)` 时间; 最坏情况, `arr` 没有 `x`, 一直遍历到结束, 用时 `O(n)`.

```c
int find(int[] arr, int n, int x) 
{
	...
	for (i = 0; i < n; ++i) 
		if (arr[i] == x)
			return i;
}
```

### 平均情况时间复杂度

上述各种执行情况时间的加权平均, 就是平均时间复杂度, average case time complexity. 对于上面的例子, 为 $O\left( \frac{n}{2} \right)\sim O(n)$, 假设每个元素为 `x` 的概率相同.

### 主定理

**主定理 (Master Theroem)** 用于求解递归算法的复杂度, 思路是将规模为 $n$ 的问题分解为 $a$ 个规模为 $\frac{n}{b}$ 的问题, 然后依次合并直至最高层, 每次合并子问题花费 $f(n)$ 时间. 其递推关系如下:

$$
T(n) = a T\left(\frac{n}{b}\right)+f(n)\qquad \forall n > b
$$

那么

$$
T(n) = \begin{cases}\Theta(n^{\log_b a}) & f(n) = O(n^{\log_b (a)-\epsilon}),\epsilon > 0 \\ \Theta(f(n)) & f(n) = \Omega(n^{\log_b (a)+\epsilon}),\epsilon\ge 0\\ \Theta(n^{\log_b a}\log^{k+1} n) & f(n)=\Theta(n^{\log_b a}\log^k n),k\ge 0 \end{cases}
$$

需要注意的是，这里的第二种情况还需要满足 regularity condition, 即对于某些常量 $c<1$ 和足够大的 $n$, 有 $a \cdot f(n/b) \leq c\cdot f(n)$.

证明略. 如 $T(n) = 2T\left(\frac{n}{2}\right) + 1$, 设: $a=2, b=2, {\log_2 2} = 1$, 那么 $\epsilon\in(0,1]$, 满足第一种情况, 因此 $T(n)=\Theta(n)$.

### 均摊时间复杂度

**摊还分析**的结果称为**均摊时间复杂度**, amortized time complexity. 对数据结构进行一系列*连续*操作, 大部分操作时间复杂度低, 个别操作时间复杂度高, 此时摊还分析尝试能否将较高时间复杂度的操作的耗时平摊到其他低时间复杂度的操作上, 从而降低整体的时间复杂度.

比如, [快速排序](排序/快速排序.md)中"按大小分类"的操作, 单次执行的最坏时间复杂度为 $O(n)$. 但由于快排算法分治的过程中, 之前的每次"分类"操作都减少了数组长度, 所以实际复杂度降为 $O(n\cdot \log n)$, 分摊在每次"分类"操作上就是 $O(\log n)$. 

#### 势能分析

势能分析是一种摊还分析方法, 用一个函数 $F$ 表达*之前操作对当前操作的影响*, 称为势能函数. $F$ 将一个状态 $S$ 映射为值 $F(S)$. 设初始状态 $S_{0}$, 那么总有 $F(S)\geq F(S_{0})$. 

定义*状态* $S$, 和初始状态 $S_0$. 通过连续 $m$ 次操作得到系统状态序列: $S_{1}, S_{2}, \cdots, S_{m}$. 设 $c_{i}$ 为第 $i$ 次操作的时间开销, 记 $p_{i}=c_{i}+F(S_{i})-F(S_{i-1})$, 那么总时间花销为

$$\sum^{m}_{i=1}c_{i}=F(S_{0})-F(S_{m})+\sum^{m}_{i=1}p_{i}$$

所以有

$$\sum^{m}_{i=1}c_{i}<\sum^{m}_{i=1}p_{i}$$

因此若 $p_{i}=O(T(n))$, 则 $O(T(n))$ 应是均摊复杂度的一个上界.