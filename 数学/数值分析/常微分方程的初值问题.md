
## IVP 问题

给定常微分方程及其初值:   

$$\begin{cases}
y'(t)  & =  & f(t, y), \quad t \in [t_{0}, T]\\ 
y(t_{0}) & = & y_{0}
\end{cases}$$

目标是求满足上式的函数 $y(t)$. 方程的解析解为: 

$$y(t) = y_0 + \int_{t_0}^t f(s, y(s))\,ds$$

当求解析解很困难时, 需要逼近该方程, 求数值解.


### 假设条件

要保证 "解存在且唯一" , 通常对右端 (f) 要求: 

1. **连续性**: (f(t,y)) 在考虑的区域内连续, 保证解存在. 
    
2. **对 (y) 满足 Lipschitz 条件**: 存在常数 (L>0), 使得  
    
    $$|f(t, y_1) - f(t, y_2)| \le L |y_1 - y_2|$$.  
    
    这保证了解的**唯一性**, 并且保证误差不会被无限放大.   
    在数值分析里, 它也是用来证明: **局部截断误差小 + Lipschitz ⇒ 全局误差有界**. 
    

 (直观理解: Lipschitz 让方程的 "斜率场" 变化不至于太猛, 系统对初值不至于过分敏感. ) 

## 单步迭代法

把区间 ([t_0, T]) 用步长 (h) 划分:   

$t_n = t_0 + nh,\quad n=0,1,\dots$

已知近似值 (y_n \approx y(t_n)), 通过一个**单步公式**计算下一点:   
 
$$y_{n+1} = y_n + h ,\Phi(t_n, y_n, h)$$,  
  
其中 (\Phi) 是对 "本步平均斜率" 的某种近似, 称为**增量函数**或**斜率组合**.   
只依赖当前点 ((t_n, y_n)) 的就是**单步法** (与多步法对比) . 

### 单步迭代法的误差

> 如果单步法的**局部截断误差**是 ($R_{n+1}=O(h^{p+1})$), 而且增量函数 ($\varphi(t,y,h)$) 对 (y) 满足 Lipschitz 条件, 那么这个单步法的**整体 (全局) 误差**就是 ($e_{n+1}=O(h^{p})$). 

这正是图里那段话要证明的结论. 

---

记单步法的统一形式:
 
$$y_{n+1} = y_n + h \varphi(t_n, y_n, h), \quad n=0,1,\dots,M-1 \tag{7.6} $$ 

其中:
- $t_{n}=t_{0}+nh$
- $y_{n}$ 是数值解
- $\varphi(t_{n},y_{n},h)$ 是增量函数. 在欧拉法里, 就代指 $f(t_{n},y_{n})$

要求: 比较迭代数值解 $y_{n}$ 和实际解 $y(t)$ 的差.

#### 2. 两种误差: 局部的和整体的

##### 2.1 整体误差 (全局误差) 

定义  
[  
$$e_{n} = y(t_n) - y_n  $$
]  
就是 "真解减去数值解" . 

##### 2.2 局部截断误差

书上定义 (对初值问题的真实解 (y(t))) :   
[  
$$R_{n+1} = y(t_{n+1}) - y(t_n) - h \varphi(t_n, y(t_n), h) \tag{7.8}$$  
]  
意思是: **假如这一小步的起点我给的是 "真" 的 (y(t_n))**, 你这个算法走一步能不能走准? 走不准的那一丢丢就是 (R_{n+1}).   
所以 (R_{n+1}) 只反映**方法本身**的逼近能力, 不掺前面积累的误差. 

---

#### 3. 把真解减去数值解: 推导误差递推式

真解满足  
[  
$$y(t_{n+1}) = y(t_n) + h \varphi(t_n, y(t_n), h) + R_{n+1} \tag{★} $$ 
]  
这是把 (7.8) 改写了一下而已. 

数值解满足 (7.6):   
[  
y_{n+1} = y_n + h \varphi(t_n, y_n, h)  
]

现在用 "真解 − 数值解" :   
[  
$$\begin{aligned}  
e_{n+1}  
&= y(t_{n+1}) - y_{n+1}\\  
&= \bigl[y(t_n) + h \varphi(t_n, y(t_n), h) + R_{n+1}\bigr] - \bigl[y_n + h \varphi(t_n, y_n, h)\bigr] \\  
&= \underbrace{y(t_n) - y_n}_{e_n} + h\bigl[\varphi(t_n, y(t_n), h) - \varphi(t_n, y_n, h)\bigr] + R_{n+1} \\  
&= e_n + h\bigl[\varphi(t_n, y(t_n), h) - \varphi(t_n, y_n, h)\bigr] + R_{n+1}.  
\end{aligned} \tag{7.7}$$  
]

这就是图里那条关键式子 (7.7). 

它的意思很直白: 

> 下一步的整体误差 = 上一步的整体误差
> 
> - 因为 "你用错了 (y)" 带来的差 (就是中间那个 (\varphi(\cdot)-\varphi(\cdot))) 
>     
> - 这一小步算法本身的本地误差 (R_{n+1}). 
>     

接下来就是把这条式子 "估计" 成一个不等式. 

---

#### 4. 用 Lipschitz 条件控制 "第二项" 

书里给了一个假设 (定义 7.1) : 

> 增量函数 (\varphi(t,y,h)) 在那个区域里对 (y) 满足 Lipschitz:   
> [  
> |\varphi(t,u_1,h) - \varphi(t,u_2,h)| \le K |u_1 - u_2|.  
> ]

这和我们平时对 (f(t,y)) 的 Lipschitz 是一回事, 只是这里函数叫 (\varphi). 

把它代进上面的误差递推式的第二项, 就有

[  
$$|\varphi(t_n, y(t_n), h) - \varphi(t_n, y_n, h)| \le K |y(t_n) - y_n| = K |e_n|.$$  
]

代回 (7.7) 取绝对值, 就得

[  
$$|e_{n+1}| \le |R_{n+1}| + |e_n| + hK|e_n|  
= |R_{n+1}| + (1 + hK)|e_n|. \tag{◎} $$ 
]

这一步非常关键: **Lipschitz 把 "方法用了错的 (y)" 这一项线性地绑到上一步误差上了. **  
所以最后我们得到的是一个**线性递推不等式**. 

---

#### 5. 展开这个递推不等式

现在我们有  
[  
$$|e_{n+1}| \le |R_{n+1}| + (1 + hK)|e_n|. $$ 
]

对 (e_n) 再套一遍同样的式子, 能得到一个 "连乘 + 累加" 的结构. 书上就是这么写的: 

[  
$$\begin{aligned}  
|e_{n+1}|  
&\le |R_{n+1}| + (1+hK)|e_n| \  
&\le |R_{n+1}| + (1+hK)\bigl(|R_n| + (1+hK)|e_{n-1}|\bigr) \  
&= |R_{n+1}| + |R_n|(1+hK) + (1+hK)^2 |e_{n-1}| \  
&\le \cdots \  
&\le \sum_{k=0}^n |R_{n+1-k}| (1+hK)^k + |e_0|(1+hK)^{n+1}.  
\end{aligned} $$ 
]

但 (e_0 = 0) (因为一开始数值解就取初值 (y_0 = y(t_0))) , 所以最后一项没了.   
于是有

[  
$$|e_{n+1}| \le \sum_{k=0}^n |R_{n+1-k}| (1+hK)^k. \tag{†} $$ 
]

这就是书中间那条 "连着点点点" 的展开. 

---

#### 6. 用 "局部截断误差的阶" 去界这串和

现在用到前面的假设: 

> 单步法的局部截断误差与 (h^{p+1}) 同阶:   
> [  
> $$R_{n+1} = O(h^{p+1}). $$ 
> ]  
> 而且这个界可以看成跟 (t_{n+1}) 有关的一个有界函数:   
> [  
> $$|R_{n+1}| \le C h^{p+1}, $$ 
> ]  
> 并且对所有步都成立 (书里记了一个 (R = \max |R_n|)) . 

把这个统一的上界记成  
  
$$|R_{j}| \le R h^{p+1}, \quad \text{所有 } j.$$  


代到 (†) 里: 

  
$$|e_{n+1}| \le \sum_{k=0}^n R h^{p+1} (1+hK)^k  
= R h^{p+1} \sum_{k=0}^n (1+hK)^k. \tag{‡} $$ 


现在要估计这个几何级数. 几何级数有公式: 

  
$$\sum_{k=0}^n (1+hK)^k = \frac{(1+hK)^{n+1} - 1}{hK}. $$ 


代回去: 

 
$$|e_{n+1}|  
\le R h^{p+1} \cdot \frac{(1+hK)^{n+1} - 1}{hK}  
= \frac{R}{K} h^{p} \bigl((1+hK)^{n+1} - 1\bigr). \tag{※}$$  


这一步和书上 182 页那几行是一样的, 只是它又把 ((1+hK)^{n+1}) 用指数估了一下. 

---

#### 7. 把 $((1+hK)^{n+1})$ 变成与区间长度无关的常数

注意: 我们是从 (t_0) 一直走到 (t_n = t_0 + nh), 所以  
[  
$$n \approx \frac{t_n - t_0}{h}.$$  
]

于是  
[  
(1+hK)^{n+1} \approx (1+hK)^{(t_n - t_0)/h}.  
]

经典估计: 当 (h \to 0) 时,   
[  
$$(1+hK)^{1/h} \to e^{K},  $$
]  
所以  
[  
$$(1+hK)^{(t_n - t_0)/h} \le e^{K (t_n - t_0)}.  $$
]

这就是书里那句  
[  
$$(1+hK)^{n+1} \le e^{(t_{n+1}-t_0)K}.  $$
]

它的好处是: **这东西跟 (h) 没关系了**, 是一个和区间长度有关, 但有界的常数. 

于是由 (※) 得到

[  
$$|e_{n+1}|  
\le \frac{R}{K} h^{p} \bigl(e^{(t_{n+1}-t_0)K} - 1\bigr)  
= O(h^{p}).  $$
]

这就是我们想要的: **整体误差和 (h^p) 同阶**. 

书最后写成  
[  
$$e_{n+1} = c(t_{n+1}) h^{p} + O(h^{p+1}), $$ 
]  
意思是前面的常数会随时间变一下, 但对于固定区间它是有界的. 




## Runge–Kutta (RK) 方法

RK 方法的核心做法: **在当前步内取若干个 "斜率样本" , 再做加权平均. **

最一般的 (s)-阶段 RK 形式:   
 
$$\begin{cases}  
k_1 &= f(t_n,, y_n), \\  
k_2 &= f\bigl(t_n + c_2 h,; y_n + h(a_{21}k_1)\bigr), \\
&\vdots \\  
k_s &= f\bigl(t_n + c_s h,; y_n + h\sum_{j=1}^{s-1} a_{sj} k_j\bigr), \\  
y_{n+1} &= y_n + h \sum_{i=1}^s b_i k_i.  
\end{cases}$$  

- $(k_i)$: 本步中不同位置的 "试探斜率" ; 
    
- $(a_{ij}, b_i, c_i)$: 方法的系数 (通常写成 Butcher 表) ; 
    
- 通过恰当选取系数, 可构造不同阶的 RK 方法. 

**欧拉法是一阶 RK 方法, 改进欧拉法是二阶 RK 方法**.

### 欧拉法

$$y_{n+1} = y_n + h f(t_n, y_n)$$

### 修正欧拉法

$$\begin{aligned}  
k_1 &= f(t_n, y_n), \  
k_2 &= f(t_n + h, y_n + hk_1), \  
y_{n+1} &= y_n + \frac{h}{2}(k_1 + k_2).  
\end{aligned}  $$


## 舍入误差

P199