## 马尔可夫模型

若系统有N个状态 $S_{1}, S_{2},\dots,S_{N}$, 随着事件 t 变化, 系统状态变量 $q_{t}$ 同样变化. t时刻状态取值 $S_{j}$ 仅取决于前 $t-1$ 个时刻的状态, 其概率为: $$P(q_{t}=S_{j}\vert q_{t-1}=S_{i},q_{t-2}=S_{k},\dots)$$

马尔可夫模型 (Markov Model) 基于上述模型做出两个**假设**:
1. 有限历史假设(无记忆性): 系统在 t 时刻状态仅与 t-1 时刻状态相关, 则该系统构成一个离散的一阶马尔可夫链: $$P(q_{t}=S_{j}\vert q_{t-1}=S_{i},q_{t-2}=S_{k},\dots)=P(q_{t}=S_{j}\vert q_{t-1}=S_{i})$$
2. 考虑 (1) 中**独立于时间t**的随机过程(不动性假设, 又称齐次性假设), 状态转移与时间无关, 则有: $$P(q_{t}=S_{j}\vert q_{t-1}=S_{i})=a_{ij},\ 1\leq i,j\leq N$$ 其中必须满足 $a_{ij}\geq 0$, $\sum^{N}_{j=1}a_{ij}=1$

马尔可夫模型又可视为**概率有限状态自动机**, 参考 [PCFG](句法分析.md):

![|250](../../attach/Pasted%20image%2020231226182823.avif)

状态序列 $S_{1},\dots, S_{T}$ 概率为: (其中 $\pi_{i}=p\ \ (q_{1}=S_{i})$ 为初始状态概率)
$$\begin{aligned} p(S_1, \dots, S_T) &= p(S_1) \times p(S_2|S_1) \times p(S_3|S_1, S_2) \times \cdots \times p(S_T|S_1, \dots, S_{T-1}) \\ &= p(S_1) \times p(S_2|S_1) \times p(S_3|S_2) \times \cdots \times p(S_T|S_{T-1}) \quad \text{(6-5)} \\ &= \pi_{S_1} \prod_{t=1}^{T-1} a_{S_t S_{t+1}} \\ \end{aligned}$$

因此图中 $p(t, i, p)=1.0\times 0.3 \times 0.6=0.18$

## 隐马尔可夫模型

隐马尔可夫模型 (Hidden Markov Model, HMM) 描述一种**双重随机过程**, 其中系统的状态不可见, 只能通过观测到的序列来推测系统状态. HMM 能够考虑时间序列数据中的时间依赖性, 并且可以不直接提供观测到的状态, 因而在序列预测和分类问题中应用广泛.

其核心组成包括:
- 状态集合 $\set{S_{1},S_{2},\dots, S_{N}}$
- 观测集合 $\set{O_{1}, O_{2}, \dots, O_{M}}$
- 状态转移概率矩阵 $A=a_{i,j}$ ($a_{ij}$ 指从状态 $S_{i}$ 转移到 $S_j$ 的概率) $$\begin{cases}
a_{i,j} & =P(q_{t+1}=S_{j}\vert q_{t}=S_{i}) \\
a_{i,j}  & \geq 0 \\
\sum^{N}_{j=1}a_{i,j} & =1
\end{cases}$$
- 观测概率矩阵 $B=b_{j}(k)$ (从状态 $S_{j}$ 观察符号 $v_{k}$ 的概率分布) $$\begin{cases}
b_{j}(k) & =p(O_{t}=v_{k}\vert q_{t}=S_{j}) \\
b_{j}(k)  & \geq 0 \\
\sum^{M}_{k=1}b_{j}(k) & =1
\end{cases}$$ 其中 $1\leq j\leq N$, $1\leq k\leq M$, $b_j(k)$ 为从状态 $S_{j}$ 观测到结果 k 的概率.
- 初始状态概率分布为 $\pi = \pi_{i}$, 其中, $$\begin{cases}
\pi_{i} &=p(q_{1}=S_{i}) \\
\pi_{i}  & \geq 0 \\
\sum^{N}_{i=1}\pi_{i} & =1
\end{cases}$$

一般将HMM记为 $\mu=(A, B, \pi)$, 给定HMM后求其观测序列 $O=O_{1}O_{2}\dots O_{T}$:
1. 令 t=1.
2. 根据初始状态分布 $\pi=\pi_{i}$, 选择初始状态 $q_{1}=S_{i}$.
3. 根据状态 $S_{i}$ 的输出概率分布 $b_{i}(k)$, 输出 $O_{t}=v_{k}$.
4. 根据状态转移概率, 转移到新状态 $q_{t+1}=S_{j}$.
5. t=t+1, 如果 $t\lt T$, 重复步骤 (3) (4); 否则结束.

```
S1 -> S2 -> S3 -> ... -> Sn
|     |     |            |
O1    O2    O3           On
```

应用中, HMM常用于解决三类问题:
1. **评估问题 Likelihood**: 给定模型 $u$ 和 O, 如何快速计算 O 的出现概率 $P(O\vert \mu)$
2. **解码问题 Decoding**: 给定模型 $\mu$ 和 O, 推断最可能的隐状态序列 $Q=q_{1}q_{2}\dots q_{T}$
3. **学习问题 Training**: 给定 O, 调整模型 $\mu$ 以最大化观测序列概率 $P(O\vert \mu)$.

对于以上问题通常用不同算法可用: (大部分基于动态规划)
- 评估问题 --> 前向算法, 后向算法
- 解码问题 --> Viterbi 搜索算法
- 学习问题 --> Baum-Welch 算法, 最大期望算法

### 前向算法

### 后向算法

### Viterbi搜索算法

### 参数学习 
