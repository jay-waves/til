语言模型:
- 语法规则模型: 归纳自然语言的语法规则
- 统计语言模型: 计算每个句子的先验概率
- 神经网络语言模型: 构建神经网络来建模自然语言内在依赖关系. (可解释性差)

## 统计语言模型

语句 $s=w_{1}w_{2}\dots w_{n}$ 的先验概率为: $$
\begin{align}
p(s) & =p(w_{1})\times p(w_{2}\vert w_{1})\times p(w_{3}\vert w_{2}w_{1})\times \dots \times p(w_{m}\vert w_{1}\dots w_{m-1}) \\
 & = \prod^{m}_{i=1}p(w_{i}\vert w_{1}\dots w_{i-1})
\end{align}$$ 当 $i=1$ 时, $p(w_1\vert w_0)=p(w_1)$; $w_i$ 称为统计基元(词), $w_1,\dots w_{i-1}$ 称为 $w_i$ 的历史.

历史存在 N^m 数据量爆炸问题, 因此引入历史等价类, 来减少历史基元个数

n元语法是一种用于从序列中预测下一个词的统计模型，称为n元语法 (n-gram). 给定一系列词 $w_1, w_2, \dots, w_N$, 我们可以根据n元语法的定义来预测下一个词。 具体地， 
- 当 $n=1$ 时, 模型仅考虑当前词 $w_i$, 这称为uni-gram或monogram;
- 当 $n=2$ 时, 2-gram (bi-gram) 模型会考虑当前词和它前面的一个词;
- 当 $n=3$ 时, 3-gram (tri-gram) 模型会考虑当前词和它前面的两个词; 以此类推.

例如, 一个 3-gram 模型会使用公式 $P(w_i | w_{i-2}, w_{i-1})$ 来预测下一个词.


> [熵](obsidian://open?vault=Math&file=%E7%A6%BB%E6%95%A3%E6%95%B0%E5%AD%A6%2F%E4%BF%A1%E6%81%AF%E8%AE%BA%2F%E7%86%B5)

对于语言 $L=(X_{i})\sim p(x)$ 与其模型 q 的交叉熵定义为: $$H(L, q)=-\lim_{ n \to \infty } \frac{1}{n}\sum_{x^{n}_{1}}p(x_{1}^{n})\log_{2}q(x_{1}^{n})$$ 其中 $x_{1}^{n}=x_{1},\dots,x_{n}$ 是语言L的语句, $p(x_{1}^{n})$ 为L中语句 $x_{1}^{n}$ 的概率, $q(x_{1}^{n})$ 为模型q对该语句的概率估计. 自然语言处理中, 设计q的目标是使模型q对语言L预测的交叉熵最小; 如果语言是理想的, 即n趋于无穷大时, 全部单词的概率之和为1, 可以近似: $$H(L, q)=-\lim_{ n \to \infty } \frac{1}{n}\log_{2}q(x_{1}^{n})$$

### 困惑度

NLP中, 困惑度 (Perplexity) 常用来代替交叉熵来评估语言模型. 给定语言 L 的样本 $x^{n}_{1}=x_{1},...,x_{2}$, L 的困惑度 $PP_{q}$ 定义为: $$PP_{q}=2^{H(L, q)}\simeq 2^{-\frac{1}{n}\log_{2}q(x^{n}_{1})}=[q(x^{n}_{1})]^{-\frac{1}{n}}$$

## 参数估计

对于 n-gram, 参数 $p(w_i\vert w^{i-1}_{i-n+1})$ 可由极大似然估计求得: $$p(w_{i}\vert w^{i-1}_{i-n+1})=f(w_{i}\vert w^{i-1}_{i-n+1})=\frac{c(w^i_{i-n+1})}{\sum_{w_{i}}c(w^i_{i-n+1})}$$ 其中 $f(w_{i}\vert w^{i-1}_{i-n+1})$ 是给定 $w^{i-1}_{i-n+1}$ 条件下的 $w_{i}$ 出现的相对频率. $\sum_{w_{i}}c(w^i_{i-n+1})$ 是历史串 $w^{i-1}_{i-n+1}$ 在给定语料中出现的次数.

## 数据平滑

数据平滑 (Data Smoothing) 用来解决参数估计中**数据匮乏 (稀疏, Sparse Data) 引起的零概率问题**.

### 加一法

加一法  (Additive Smoothing). 给定二元语法, 对语料中任意 $w_{i}w_{j}$ 词对添加一次出现次数: $$p(w_{i}\vert w_{i-1})=\frac{1+c(w_{i-1}w_{i})}{\sum_{w_{j}}[1+c(w_{i-1}w_{j})]}$$

### 减值法

又称折扣法 (Discounting): 修改训练样本中事件的实际计数, 使样本中不同事件的概率之和小于1, 剩余概率分配给未见事件.

#### 1 Good-Turing法

对非0事件**按公式削减**出现次数, 余出概率**均分**给0概率事件.

#### 2 Katz后退法

对非0事件按Good-Turing法减值, 余出概率**按低阶分布分配**给0概率事件.

#### 3 绝对减值法

对非0事件无条件**削减某一固定出现次数**, 余出部分**均分**给0概率事件.

#### 4 线性减值法

对非0事件**按比例削减次数值**, 余出部分**概率均分**该0概率事件.

### 删除插值法

删除插值法 Deleted Interpolation

## 语言模型的自适应

## 注意力机制